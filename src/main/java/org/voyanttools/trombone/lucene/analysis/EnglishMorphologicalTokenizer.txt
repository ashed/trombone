/*******************************************************************************
 * Trombone is a flexible text processing and analysis library used
 * primarily by Voyant Tools (voyant-tools.org).
 * 
 * Copyright (©) 2007-2012 Stéfan Sinclair & Geoffrey Rockwell
 * 
 * This file is part of Trombone.
 * 
 * Trombone is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 * 
 * Trombone is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with Trombone.  If not, see <http://www.gnu.org/licenses/>.
 ******************************************************************************/
package org.voyanttools.trombone.lucene.analysis;

import java.io.IOException;
import java.io.Reader;
import java.util.Collection;
import java.util.Iterator;

import static org.uimafit.factory.AnalysisEngineFactory.createAggregate;
import static org.uimafit.factory.AnalysisEngineFactory.createAggregateDescription;
import static org.uimafit.factory.AnalysisEngineFactory.createPrimitiveDescription;
import static org.uimafit.util.JCasUtil.select;

import org.apache.commons.io.IOUtils;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
import org.apache.uima.UIMAException;
import org.apache.uima.analysis_engine.AnalysisEngine;
import org.apache.uima.analysis_engine.AnalysisEngineDescription;
import org.apache.uima.analysis_engine.AnalysisEngineProcessException;
import org.apache.uima.jcas.JCas;
import org.apache.uima.resource.ResourceInitializationException;
import org.uimafit.factory.JCasFactory;

import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token;
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.StanfordSegmenter;
import de.tudarmstadt.ukp.dkpro.core.treetagger.TreeTaggerPosLemmaTT4J;

/**
 * @author Stéfan Sinclair, Richard Eckart
 */
public class EnglishMorphologicalTokenizer extends Tokenizer {

	private PositionIncrementAttribute posIncr;
	private CharTermAttribute termAtt;
	private OffsetAttribute offsetAttribute;
	private Iterator<Token> tokensIterator;
	
	/**
	 * 
	 */
	public EnglishMorphologicalTokenizer(Reader reader) {
		super(reader);
		
		posIncr = addAttribute(PositionIncrementAttribute.class);
		termAtt = addAttribute(CharTermAttribute.class);
		offsetAttribute = addAttribute(OffsetAttribute.class);
		
		JCas jcas;
		try {
			jcas = JCasFactory.createJCas();
		} catch (UIMAException e) {
			throw new RuntimeException("Unable to instantiate document for lemmatization", e);
		}
		
		jcas.setDocumentLanguage("en");
		
		String string;
		try {
			string = IOUtils.toString(reader);
		}
		catch (IOException e) {
			throw new RuntimeException("Unable to get content during lemmatization.", e);
		}
		jcas.setDocumentText(string);
		
		AnalysisEngineDescription tokenizer;
		AnalysisEngineDescription filter;
		AnalysisEngineDescription treetagger;
		AnalysisEngine engine;
		try {
			tokenizer = createPrimitiveDescription(StanfordSegmenter.class);
			filter = createPrimitiveDescription(MarkupFilter.class);
			treetagger = createPrimitiveDescription(TreeTaggerPosLemmaTT4J.class);
			engine = createAggregate(createAggregateDescription(tokenizer, filter, treetagger));
		} catch (ResourceInitializationException e) {
			throw new RuntimeException("Unable to initialize a needed analysis engine during lemmatization.", e);
		}
		
		try {
			engine.process(jcas);
		} catch (AnalysisEngineProcessException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		Collection<Token> tokens = select(jcas, Token.class);
		tokensIterator = tokens.iterator();
		
	}

	@Override
	public boolean incrementToken() throws IOException {
		clearAttributes();
		Token token = tokensIterator.next();
		termAtt.setEmpty();
		String lemma = token.getLemma().getValue();
		termAtt.append(lemma);
		termAtt.setLength(lemma.length());
		offsetAttribute.setOffset(token.getBegin(), token.getEnd());
		posIncr.setPositionIncrement(1);
		return tokensIterator.hasNext();
	}
	
	public void end() {
//		System.err.println(offsetAttribute.endOffset());
	}

}
